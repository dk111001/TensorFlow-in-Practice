{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "print(training_images.shape)\n",
    "training_images=training_images / 255.0\n",
    "\n",
    "test_images=test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCallback(keras.callbacks.Callback):\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"\\nEnd epoch {} of training; got log keys: {}\\n\".format(epoch, keys))\n",
    "        print(logs[\"accuracy\"])\n",
    "        if(logs[\"accuracy\"]>0.998):\n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_18 (Conv2D)           (None, 26, 26, 64)        640       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 11, 11, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 128)               204928    \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 243,786\n",
      "Trainable params: 243,786\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x0000018FF2496438> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x0000018FF2496438> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.1263 - accuracy: 0.9605\n",
      "End epoch 0 of training; got log keys: ['loss', 'accuracy']\n",
      "\n",
      "0.9605333\n",
      "60000/60000 [==============================] - 10s 167us/sample - loss: 0.1262 - accuracy: 0.9605\n",
      "Epoch 2/20\n",
      "59712/60000 [============================>.] - ETA: 0s - loss: 0.0394 - accuracy: 0.9883\n",
      "End epoch 1 of training; got log keys: ['loss', 'accuracy']\n",
      "\n",
      "0.98828334\n",
      "60000/60000 [==============================] - 9s 153us/sample - loss: 0.0393 - accuracy: 0.9883\n",
      "Epoch 3/20\n",
      "59648/60000 [============================>.] - ETA: 0s - loss: 0.0265 - accuracy: 0.9918\n",
      "End epoch 2 of training; got log keys: ['loss', 'accuracy']\n",
      "\n",
      "0.9917833\n",
      "60000/60000 [==============================] - 9s 154us/sample - loss: 0.0265 - accuracy: 0.9918\n",
      "Epoch 4/20\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: 0.0191 - accuracy: 0.9938\n",
      "End epoch 3 of training; got log keys: ['loss', 'accuracy']\n",
      "\n",
      "0.99385\n",
      "60000/60000 [==============================] - 9s 152us/sample - loss: 0.0191 - accuracy: 0.9938\n",
      "Epoch 5/20\n",
      "59872/60000 [============================>.] - ETA: 0s - loss: 0.0130 - accuracy: 0.9957\n",
      "End epoch 4 of training; got log keys: ['loss', 'accuracy']\n",
      "\n",
      "0.9957\n",
      "60000/60000 [==============================] - 9s 152us/sample - loss: 0.0130 - accuracy: 0.9957\n",
      "Epoch 6/20\n",
      "59712/60000 [============================>.] - ETA: 0s - loss: 0.0116 - accuracy: 0.9964\n",
      "End epoch 5 of training; got log keys: ['loss', 'accuracy']\n",
      "\n",
      "0.9964667\n",
      "60000/60000 [==============================] - 9s 151us/sample - loss: 0.0115 - accuracy: 0.9965\n",
      "Epoch 7/20\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0100 - accuracy: 0.9970\n",
      "End epoch 6 of training; got log keys: ['loss', 'accuracy']\n",
      "\n",
      "0.99696666\n",
      "60000/60000 [==============================] - 9s 154us/sample - loss: 0.0100 - accuracy: 0.9970\n",
      "Epoch 8/20\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: 0.0078 - accuracy: 0.9976\n",
      "End epoch 7 of training; got log keys: ['loss', 'accuracy']\n",
      "\n",
      "0.9976\n",
      "60000/60000 [==============================] - 9s 156us/sample - loss: 0.0078 - accuracy: 0.9976\n",
      "Epoch 9/20\n",
      "59904/60000 [============================>.] - ETA: 0s - loss: 0.0075 - accuracy: 0.9975\n",
      "End epoch 8 of training; got log keys: ['loss', 'accuracy']\n",
      "\n",
      "0.9975\n",
      "60000/60000 [==============================] - 9s 155us/sample - loss: 0.0075 - accuracy: 0.9975\n",
      "Epoch 10/20\n",
      "59840/60000 [============================>.] - ETA: 0s - loss: 0.0058 - accuracy: 0.9981\n",
      "End epoch 9 of training; got log keys: ['loss', 'accuracy']\n",
      "\n",
      "0.99808335\n",
      "60000/60000 [==============================] - 9s 155us/sample - loss: 0.0058 - accuracy: 0.9981\n",
      "10000/1 - 1s - loss: 0.0162 - accuracy: 0.9928\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "training_images=training_images.reshape(60000, 28, 28, 1)\n",
    "training_images=training_images / 255.0\n",
    "test_images = test_images.reshape(10000, 28, 28, 1)\n",
    "test_images=test_images/255.0\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
    "  tf.keras.layers.MaxPooling2D(2,2),\n",
    "  tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
    "  tf.keras.layers.MaxPooling2D(2,2),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(training_images, training_labels, epochs=20,callbacks = [CustomCallback()])\n",
    "test_loss = model.evaluate(test_images, test_labels,verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "            # YOUR CODE STARTS HERE\n",
    "            tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
    "            tf.keras.layers.MaxPooling2D(2, 2),\n",
    "            tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "            tf.keras.layers.MaxPooling2D(2,2),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.Dense(10, activation='softmax')\n",
    "            # YOUR CODE ENDS HERE\n",
    "            ])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/12\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x00000190B65DA4C8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x00000190B65DA4C8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "60000/60000 [==============================] - 10s 167us/sample - loss: 0.1207 - accuracy: 0.9627\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 9s 155us/sample - loss: 0.0413 - accuracy: 0.9872\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 9s 156us/sample - loss: 0.0283 - accuracy: 0.9912\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 9s 150us/sample - loss: 0.0194 - accuracy: 0.9939\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 12s 192us/sample - loss: 0.0157 - accuracy: 0.9944\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 13s 223us/sample - loss: 0.0115 - accuracy: 0.9962- loss: 0.011\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 15s 245us/sample - loss: 0.0097 - accuracy: 0.9968\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 11s 186us/sample - loss: 0.0073 - accuracy: 0.9977\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 12s 194us/sample - loss: 0.0071 - accuracy: 0.9978\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 11s 181us/sample - loss: 0.0060 - accuracy: 0.9982\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 12s 203us/sample - loss: 0.0060 - accuracy: 0.9980\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 14s 231us/sample - loss: 0.0044 - accuracy: 0.9986\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x18ff2aa2908>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "        # YOUR CODE STARTS HERE\n",
    "            training_images, training_labels, epochs=12\n",
    "        # YOUR CODE ENDS HERE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mnist_conv():\n",
    "    # Please write your code only where you are indicated.\n",
    "    # please do not remove model fitting inline comments.\n",
    "\n",
    "    # YOUR CODE STARTS HERE\n",
    "    class myCallback(tf.keras.callbacks.Callback):\n",
    "        def on_epoch_end(self, epoch, logs={}):\n",
    "            if(logs['accuracy']>0.998):\n",
    "                print(\"\\nReached 99.8% accuracy so cancelling training!\")\n",
    "                self.model.stop_training = True\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "\n",
    "    mnist = tf.keras.datasets.mnist\n",
    "    (training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "    # YOUR CODE STARTS HERE\n",
    "    callbacks = myCallback()\n",
    "    \n",
    "    training_images=training_images.reshape(60000, 28, 28, 1)\n",
    "    training_images=training_images / 255.0\n",
    "    test_images = test_images.reshape(10000, 28, 28, 1)\n",
    "    test_images=test_images/255.0\n",
    "    # YOUR CODE ENDS HERE\n",
    "\n",
    "    model = tf.keras.models.Sequential([\n",
    "            # YOUR CODE STARTS HERE\n",
    "            tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
    "            tf.keras.layers.MaxPooling2D(2, 2),\n",
    "            tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "            tf.keras.layers.MaxPooling2D(2,2),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.Dense(10, activation='softmax')\n",
    "            # YOUR CODE ENDS HERE\n",
    "            ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    # model fitting\n",
    "    history = model.fit(\n",
    "        # YOUR CODE STARTS HERE\n",
    "            training_images, training_labels, epochs=20, callbacks=[callbacks]\n",
    "        # YOUR CODE ENDS HERE\n",
    "    )\n",
    "    # model fitting\n",
    "    return history.epoch, history.history['accuracy'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x0000018FF2E834C8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x0000018FF2E834C8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "60000/60000 [==============================] - 10s 169us/sample - loss: 0.1209 - accuracy: 0.9627\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 9s 155us/sample - loss: 0.0413 - accuracy: 0.9871\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 10s 159us/sample - loss: 0.0271 - accuracy: 0.9914\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 9s 155us/sample - loss: 0.0203 - accuracy: 0.9933\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 9s 154us/sample - loss: 0.0149 - accuracy: 0.9954\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 9s 156us/sample - loss: 0.0122 - accuracy: 0.9961\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 9s 156us/sample - loss: 0.0097 - accuracy: 0.9968\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 9s 156us/sample - loss: 0.0082 - accuracy: 0.9973\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 9s 158us/sample - loss: 0.0063 - accuracy: 0.9979\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 9s 155us/sample - loss: 0.0066 - accuracy: 0.9979\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 9s 154us/sample - loss: 0.0063 - accuracy: 0.9978\n",
      "Epoch 12/20\n",
      "59968/60000 [============================>.] - ETA: 0s - loss: 0.0042 - accuracy: 0.9987\n",
      "Reached 99.8% accuracy so cancelling training!\n",
      "60000/60000 [==============================] - 9s 155us/sample - loss: 0.0042 - accuracy: 0.9987\n"
     ]
    }
   ],
   "source": [
    "_, _ = train_mnist_conv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
